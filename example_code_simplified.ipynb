{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install --upgrade scikit-learn --user\n",
    "# ! pip install missingno --user\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Modelling Algorithms\n",
    "## Classification\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "\n",
    "## Regression\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor, GradientBoostingRegressor, AdaBoostRegressor \n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Modelling Helpers\n",
    "from sklearn.impute import SimpleImputer as Imputer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder, Normalizer, scale\n",
    "\n",
    "# Evaluation metrics\n",
    "## Regression\n",
    "from sklearn.metrics import mean_squared_log_error, mean_squared_error, r2_score, mean_absolute_error \n",
    "\n",
    "## Classification\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score  \n",
    "\n",
    "# Visualization\n",
    "# import matplotlib as mpl\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.pylab as pylab\n",
    "# import seaborn as sns\n",
    "# import missingno as msno\n",
    "# from IPython.core.display import HTML\n",
    "\n",
    "# Import the data\n",
    "df = pd.read_csv('CSV/diamonds.csv')\n",
    "diamonds = df.copy()\n",
    "\n",
    "# Format data / prepare for use\n",
    "## Drop Unnamed:0 column\n",
    "diamonds = diamonds.drop(columns=\"Unnamed: 0\")\n",
    "\n",
    "# Remove rows with dimensions of '0'\n",
    "diamonds = diamonds[(diamonds[['x','y','z']] != 0).all(axis=1)]\n",
    "\n",
    "# Create new column called Volume\n",
    "diamonds['volume'] = diamonds['x']*diamonds['y']*diamonds['z']\n",
    "\n",
    "# Scale and Train\n",
    "diamonds.drop(['x','y','z'], axis=1, inplace=True)\n",
    "label_cut = LabelEncoder()\n",
    "label_color = LabelEncoder()\n",
    "label_clarity = LabelEncoder()\n",
    "\n",
    "diamonds['cut'] = label_cut.fit_transform(diamonds['cut'])\n",
    "diamonds['color'] = label_color.fit_transform(diamonds['color'])\n",
    "diamonds['clarity'] = label_clarity.fit_transform(diamonds['clarity'])\n",
    "\n",
    "# Split the data into train and test.\n",
    "X = diamonds.drop(['price'], axis=1)\n",
    "y = diamonds['price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=66)\n",
    "\n",
    "# Applying Feature Scaling ( StandardScaler )\n",
    "# You can also Apply MinMaxScaler.\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# Collect all R2 Scores.\n",
    "R2_Scores = []\n",
    "models = ['Linear Regression', 'Lasso Regression', 'AdaBoost Regression', \n",
    "          'Ridge Regression', 'GradientBoosting Regression',\n",
    "          'RandomForest Regression', 'KNeighbours Regression']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Regression Models</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    7.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1    14009477.5296            1.19s\n",
      "         2    12437807.7359            1.08s\n",
      "         3    11113339.5845            1.03s\n",
      "         4     9945244.2308            1.01s\n",
      "         5     8973416.9156            1.01s\n",
      "         6     8109014.7842            0.99s\n",
      "         7     7387120.0500            0.97s\n",
      "         8     6753937.9878            0.97s\n",
      "         9     6197182.6819            0.95s\n",
      "        10     5724689.0901            0.94s\n",
      "        20     3200362.4597            0.84s\n",
      "        30     2393542.3170            0.73s\n",
      "        40     2102586.3335            0.62s\n",
      "        50     1923964.9187            0.52s\n",
      "        60     1790574.6006            0.41s\n",
      "        70     1688380.2826            0.31s\n",
      "        80     1609829.0076            0.21s\n",
      "        90     1548089.0039            0.10s\n",
      "       100     1499127.4566            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1    13994442.1962            0.89s\n",
      "         2    12429322.7982            0.83s\n",
      "         3    11112606.0983            0.81s\n",
      "         4     9944843.0686            0.79s\n",
      "         5     8977395.9870            0.78s\n",
      "         6     8111748.5741            0.77s\n",
      "         7     7395490.7272            0.77s\n",
      "         8     6765223.5285            0.76s\n",
      "         9     6204866.4570            0.75s\n",
      "        10     5734465.9748            0.74s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        20     3206145.1577            0.67s\n",
      "        30     2394369.2846            0.60s\n",
      "        40     2101114.6326            0.51s\n",
      "        50     1921108.4005            0.42s\n",
      "        60     1785959.4111            0.34s\n",
      "        70     1683385.7302            0.26s\n",
      "        80     1604163.5538            0.17s\n",
      "        90     1542370.2912            0.09s\n",
      "       100     1493476.7608            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1    14044115.9884            0.89s\n",
      "         2    12472837.6750            0.88s\n",
      "         3    11137657.6396            0.84s\n",
      "         4     9974212.6419            0.82s\n",
      "         5     8994369.5031            0.82s\n",
      "         6     8133396.8459            0.81s\n",
      "         7     7407925.9669            0.80s\n",
      "         8     6764110.5537            0.79s\n",
      "         9     6215416.1793            0.79s\n",
      "        10     5736700.1166            0.77s\n",
      "        20     3210108.0310            0.68s\n",
      "        30     2402276.2056            0.60s\n",
      "        40     2112221.2275            0.51s\n",
      "        50     1934266.1687            0.42s\n",
      "        60     1801087.0287            0.34s\n",
      "        70     1699719.1554            0.25s\n",
      "        80     1621327.8312            0.17s\n",
      "        90     1559382.1164            0.08s\n",
      "       100     1510393.9635            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1    14049930.2441            0.89s\n",
      "         2    12464124.5936            0.88s\n",
      "         3    11134339.1520            0.84s\n",
      "         4     9963572.7604            0.82s\n",
      "         5     8988544.3119            0.80s\n",
      "         6     8123782.2835            0.78s\n",
      "         7     7389901.0249            0.77s\n",
      "         8     6746492.7030            0.77s\n",
      "         9     6199732.4929            0.76s\n",
      "        10     5719212.8946            0.75s\n",
      "        20     3190875.3245            0.66s\n",
      "        30     2381512.2819            0.58s\n",
      "        40     2090340.2810            0.49s\n",
      "        50     1911382.9450            0.41s\n",
      "        60     1777779.4025            0.33s\n",
      "        70     1675708.2272            0.25s\n",
      "        80     1597212.1456            0.16s\n",
      "        90     1535230.7915            0.08s\n",
      "       100     1486232.9351            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1    13979667.1721            0.89s\n",
      "         2    12410196.9258            0.83s\n",
      "         3    11091464.1339            0.84s\n",
      "         4     9924417.4531            0.82s\n",
      "         5     8957051.8356            0.80s\n",
      "         6     8090860.3178            0.80s\n",
      "         7     7375141.7273            0.78s\n",
      "         8     6738456.6139            0.78s\n",
      "         9     6185985.1013            0.77s\n",
      "        10     5710402.7142            0.76s\n",
      "        20     3187460.0845            0.70s\n",
      "        30     2381173.1454            0.62s\n",
      "        40     2090773.7598            0.52s\n",
      "        50     1911732.9770            0.44s\n",
      "        60     1778590.7605            0.35s\n",
      "        70     1677144.9024            0.26s\n",
      "        80     1598482.5518            0.17s\n",
      "        90     1537106.7445            0.09s\n",
      "       100     1488486.3117            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1    13978748.2331            0.89s\n",
      "         2    12405054.9778            0.83s\n",
      "         3    11080465.6241            0.84s\n",
      "         4     9914747.0919            0.82s\n",
      "         5     8945923.9930            0.80s\n",
      "         6     8080995.1785            0.80s\n",
      "         7     7359121.7076            0.78s\n",
      "         8     6730987.4249            0.78s\n",
      "         9     6173506.2064            0.78s\n",
      "        10     5705021.9472            0.76s\n",
      "        20     3193418.0981            0.68s\n",
      "        30     2392723.0847            0.60s\n",
      "        40     2103994.3744            0.51s\n",
      "        50     1925922.2525            0.43s\n",
      "        60     1792394.0684            0.34s\n",
      "        70     1690611.3128            0.26s\n",
      "        80     1611922.8661            0.17s\n",
      "        90     1550358.7743            0.09s\n",
      "       100     1501582.8989            0.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    4.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   48.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    1.1s finished\n"
     ]
    }
   ],
   "source": [
    "def return_r2(y_test, y_pred):\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = mean_squared_error(y_test, y_pred)**0.5\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    return r2, mse, mae, rmse\n",
    "\n",
    "\"\"\" Linear Regression \"\"\"\n",
    "clf_lr = LinearRegression()\n",
    "clf_lr.fit(X_train , y_train)\n",
    "accuracies = cross_val_score(estimator = clf_lr, X = X_train, y = y_train, cv = 5,verbose = 1)\n",
    "y_pred = clf_lr.predict(X_test)\n",
    "\n",
    "# Use function to return r2\n",
    "r2, mse, mae, rmse = return_r2(y_test, y_pred)\n",
    "\n",
    "# Append to R2_Scores\n",
    "R2_Scores.append(r2)\n",
    "\n",
    "\"\"\" Lasso Regression \"\"\"\n",
    "clf_la = Lasso()\n",
    "clf_la.fit(X_train , y_train)\n",
    "accuracies = cross_val_score(estimator = clf_la, X = X_train, y = y_train, cv = 5,verbose = 1)\n",
    "y_pred = clf_la.predict(X_test)\n",
    "\n",
    "# Use function to return r2\n",
    "r2, mse, mae, rmse = return_r2(y_test, y_pred)\n",
    "\n",
    "# Append to R2_Scores\n",
    "R2_Scores.append(r2)\n",
    "\n",
    "\"\"\" AdaBoostRegressor \"\"\"\n",
    "clf_ar = AdaBoostRegressor(n_estimators=1000)\n",
    "clf_ar.fit(X_train , y_train)\n",
    "accuracies = cross_val_score(estimator = clf_ar, X = X_train, y = y_train, cv = 5,verbose = 1)\n",
    "y_pred = clf_ar.predict(X_test)\n",
    "\n",
    "# Use function to return r2\n",
    "r2, mse, mae, rmse = return_r2(y_test, y_pred)\n",
    "\n",
    "# Append to R2_Scores\n",
    "R2_Scores.append(r2)\n",
    "\n",
    "\"\"\" Ridge Regression \"\"\"\n",
    "clf_rr = Ridge()\n",
    "clf_rr.fit(X_train , y_train)\n",
    "accuracies = cross_val_score(estimator = clf_rr, X = X_train, y = y_train, cv = 5,verbose = 1)\n",
    "y_pred = clf_rr.predict(X_test)\n",
    "\n",
    "# Use function to return r2\n",
    "r2, mse, mae, rmse = return_r2(y_test, y_pred)\n",
    "\n",
    "# Append to R2_Scores\n",
    "R2_Scores.append(r2)\n",
    "\n",
    "\"\"\" Gradient Boosting Regression \"\"\"\n",
    "clf_gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,max_depth=1, random_state=0, loss='squared_error',verbose = 1)\n",
    "clf_gbr.fit(X_train , y_train)\n",
    "accuracies = cross_val_score(estimator = clf_gbr, X = X_train, y = y_train, cv = 5,verbose = 1)\n",
    "y_pred = clf_gbr.predict(X_test)\n",
    "\n",
    "# Use function to return r2\n",
    "r2, mse, mae, rmse = return_r2(y_test, y_pred)\n",
    "\n",
    "# Append to R2_Scores\n",
    "R2_Scores.append(r2)\n",
    "\n",
    "\"\"\" Random Forest \"\"\"\n",
    "clf_rf = RandomForestRegressor()\n",
    "clf_rf.fit(X_train , y_train)\n",
    "accuracies = cross_val_score(estimator = clf_rf, X = X_train, y = y_train, cv = 5,verbose = 1)\n",
    "y_pred = clf_rf.predict(X_test)\n",
    "\n",
    "# Fine Tune Random Forest\n",
    "no_of_test=[100]\n",
    "params_dict={'n_estimators':no_of_test,'n_jobs':[-1],'max_features':[\"auto\",'sqrt','log2']}\n",
    "clf_rf=GridSearchCV(estimator=RandomForestRegressor(),param_grid=params_dict,scoring='r2')\n",
    "clf_rf.fit(X_train,y_train)\n",
    "\n",
    "pred=clf_rf.predict(X_test)\n",
    "\n",
    "# Use function to return r2\n",
    "r2, mse, mae, rmse = return_r2(y_test, pred)\n",
    "\n",
    "# Append to R2_Scores\n",
    "R2_Scores.append(r2)\n",
    "\n",
    "\"\"\" KNeighbors Regression \"\"\"\n",
    "clf_knn = KNeighborsRegressor()\n",
    "clf_knn.fit(X_train , y_train)\n",
    "accuracies = cross_val_score(estimator = clf_knn, X = X_train, y = y_train, cv = 5,verbose = 1)\n",
    "y_pred = clf_knn.predict(X_test)\n",
    "\n",
    "# Fine Tune KNeighbors\n",
    "n_neighbors=[]\n",
    "for i in range (0,50,5):\n",
    "    if(i!=0):\n",
    "        n_neighbors.append(i)\n",
    "params_dict={'n_neighbors':n_neighbors,'n_jobs':[-1]}\n",
    "clf_knn=GridSearchCV(estimator=KNeighborsRegressor(),param_grid=params_dict,scoring='r2')\n",
    "clf_knn.fit(X_train,y_train)\n",
    "\n",
    "pred=clf_knn.predict(X_test)\n",
    "\n",
    "# Use function to return r2\n",
    "r2, mse, mae, rmse = return_r2(y_test, pred)\n",
    "\n",
    "# Append to R2_Scores\n",
    "R2_Scores.append(r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithms</th>\n",
       "      <th>R2-Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RandomForest Regression</td>\n",
       "      <td>0.978909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KNeighbours Regression</td>\n",
       "      <td>0.959028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AdaBoost Regression</td>\n",
       "      <td>0.906078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GradientBoosting Regression</td>\n",
       "      <td>0.905833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ridge Regression</td>\n",
       "      <td>0.881433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>0.881432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>0.881431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Algorithms  R2-Scores\n",
       "5      RandomForest Regression   0.978909\n",
       "6       KNeighbours Regression   0.959028\n",
       "2          AdaBoost Regression   0.906078\n",
       "4  GradientBoosting Regression   0.905833\n",
       "3             Ridge Regression   0.881433\n",
       "0            Linear Regression   0.881432\n",
       "1             Lasso Regression   0.881431"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate\n",
    "# Visualizing R2-Score of Algorithms\n",
    "compare = pd.DataFrame({'Algorithms' : models , 'R2-Scores' : R2_Scores})\n",
    "compare.sort_values(by='R2-Scores' ,ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
